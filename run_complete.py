# Relative Path: run_complete.py
#!/usr/bin/env python3
"""
å®Œæ•´è¿è¡Œè„šæœ¬
è¿è¡Œå®Œæ•´çš„å·¥æ§è‡ªå­¦ä¹ æµç¨‹ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€éªŒè¯å’ŒæŠ¥å‘Šç”Ÿæˆ
"""

import yaml
import json
import os
from datetime import datetime
from typing import Dict, Any

from simulator.data_generator import TrafficGenerator
from simulator.packet_parser import PacketParser
from simulator.learner.comm_learner import CommunicationLearner
from simulator.learner.value_learner import ValueLearner
from simulator.model.models import PacketMetadata, ProtocolData, LearningContext
from simulator.model.database import ObservationDatabase

def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def run_complete_pipeline():
    """è¿è¡Œå®Œæ•´çš„è‡ªå­¦ä¹ æµç¨‹"""
    print("ğŸ­ å·¥æ§è‡ªå­¦ä¹ ç³»ç»Ÿ - å®Œæ•´è¿è¡Œ")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    config = load_config()
    print(f"âš™ï¸  ä½¿ç”¨é…ç½®: config.yaml")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("outputs", exist_ok=True)
    
    # åˆå§‹åŒ–å­¦ä¹ ä¸Šä¸‹æ–‡ï¼ˆ2å¤©å­¦ä¹ ï¼‰
    duration_days = config.get('learning', {}).get('duration_days', 2)
    context = LearningContext(
        mode='training',
        start_time=datetime.now(),
        duration_days=duration_days,
        min_observation_count=config.get('learning', {}).get('min_observation_count', 10),
        min_observation_days=config.get('learning', {}).get('min_observation_days', 2)
    )
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = ObservationDatabase()
    
    # åˆå§‹åŒ–å­¦ä¹ å™¨
    packet_parser = PacketParser(config)
    comm_learner = CommunicationLearner(config, context, db)
    value_learner = ValueLearner(config, context, packet_parser, db)
    
    # åˆå§‹åŒ–æµé‡ç”Ÿæˆå™¨
    generator = TrafficGenerator(config)
    
    print(f"ğŸš€ å¼€å§‹å­¦ä¹ é˜¶æ®µï¼ˆ{duration_days}å¤©æ¨¡æ‹Ÿï¼‰")
    print("-" * 40)
    
    # æ¨¡æ‹Ÿå¤šå¤©çš„æµé‡ï¼ˆå®é™…è¿è¡Œæ—¶é—´ä¼šæ›´å¿«ï¼‰
    total_hours = duration_days * 24
    batch_minutes = 60  # æ¯æ¬¡å¤„ç†1å°æ—¶çš„æµé‡
    
    processed_hours = 0
    while processed_hours < total_hours:
        # ç”Ÿæˆ1å°æ—¶æµé‡
        packets = generator.generate_traffic_batch(batch_minutes)
        
        # å¤„ç†æ¯ä¸ªæ•°æ®åŒ…
        for packet_meta, proto_data in packets:
            # é€šä¿¡å­¦ä¹ å™¨å­¦ä¹ 
            comm_learner.learn(packet_meta, proto_data)
            
            # å€¼åŸŸå­¦ä¹ å™¨å­¦ä¹ 
            value_learner.learn(packet_meta, proto_data)
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context.total_packets_processed += len(packets)
        context.total_sessions_observed += len([p for p in packets if p[1] is not None])
        
        # æ›´æ–°è¿›åº¦
        processed_hours += 1
        progress = processed_hours / total_hours * 100
        if processed_hours % 10 == 0 or processed_hours == total_hours:  # æ¯10å°æ—¶æˆ–ç»“æŸæ—¶æ‰“å°ä¸€æ¬¡
            print(f"ğŸ“Š è¿›åº¦: {progress:.1f}% ({processed_hours}/{total_hours}) "
                  f"- å·²å¤„ç† {context.total_packets_processed} ä¸ªæ•°æ®åŒ…")
    
    print("\nâœ… å­¦ä¹ é˜¶æ®µå®Œæˆ")
    
    # å®Œæˆå­¦ä¹ 
    comm_result = comm_learner.finalize_learning()
    value_result = value_learner.finalize_learning()
    
    print(f"ğŸ“ˆ é€šä¿¡å­¦ä¹ ç»“æœ: {comm_result['total_connections']} ä¸ªè¿æ¥, "
          f"{comm_result['approved_connections']} ä¸ªæ‰¹å‡†")
    print(f"ğŸ“ˆ å€¼åŸŸå­¦ä¹ ç»“æœ: {value_result['total_parameters']} ä¸ªå‚æ•°, "
          f"{value_result['valid_models']} ä¸ªæœ‰æ•ˆæ¨¡å‹")
    
    # ç”Ÿæˆå®Œæ•´ç™½åå•
    print("\nğŸ“‹ ç”Ÿæˆå®Œæ•´ç™½åå•...")
    whitelist = {
        'generated_at': datetime.now().isoformat(),
        'learning_duration_days': duration_days,
        'total_packets_processed': context.total_packets_processed,
        'communication_rules': [],
        'value_rules': []
    }
    
    # æ·»åŠ æ‰¹å‡†çš„è¿æ¥åˆ°ç™½åå•
    approved_connections = comm_learner.get_approved_connections()
    for conn in approved_connections:
        whitelist['communication_rules'].append({
            'src_ip': conn.src_ip,
            'dst_ip': conn.dst_ip,
            'dst_port': conn.dst_port,
            'protocol': conn.protocol,
            'first_observed': conn.first_observed.isoformat(),
            'last_observed': conn.last_observed.isoformat(),
            'observation_count': conn.observation_count,
            'avg_packets_per_day': conn.avg_packets_per_day,
            'max_packets_per_minute': conn.max_packets_per_minute,
            'confidence': conn.confidence,
            'rejection_reason': conn.rejection_reason
        })
    
    # æ·»åŠ å€¼åŸŸè§„åˆ™
    for (protocol, address), value_obs in value_learner.value_observations.items():
        if value_obs.observation_count >= value_learner.min_value_observations and value_obs.baseline_min is not None:
            whitelist['value_rules'].append({
                'address': value_obs.address,
                'data_type': value_obs.data_type,
                'tag_name': value_obs.tag_name,
                'unit': value_obs.unit,
                'min_value': value_obs.baseline_min,
                'max_value': value_obs.baseline_max,
                'mean': value_obs.mean,
                'std_dev': value_obs.std_dev,
                'tolerance': value_obs.tolerance,
                'observation_count': value_obs.observation_count
            })
    
    # ä¿å­˜ç™½åå•
    whitelist_path = "outputs/whitelist.yaml"
    with open(whitelist_path, 'w', encoding='utf-8') as f:
        import yaml
        yaml.dump(whitelist, f, default_flow_style=False, allow_unicode=True)
    
    print(f"ğŸ’¾ å®Œæ•´ç™½åå•å·²ä¿å­˜åˆ° {whitelist_path}")
    print(f"   - {len(whitelist['communication_rules'])} ä¸ªé€šä¿¡è§„åˆ™")
    print(f"   - {len(whitelist['value_rules'])} ä¸ªå€¼åŸŸè§„åˆ™")
    
    # è¿è¡ŒéªŒè¯æµ‹è¯•
    print("\nğŸ” è¿è¡ŒéªŒè¯æµ‹è¯•...")
    
    # ç”Ÿæˆæ­£å¸¸æµ‹è¯•æµé‡
    normal_test_packets = generator.generate_traffic_batch(60)  # 1å°æ—¶æ­£å¸¸æµé‡
    
    # ç”Ÿæˆæ”»å‡»æµ‹è¯•æµé‡
    recon_attack = generator.generate_attack_traffic("recon")
    dos_attack = generator.generate_attack_traffic("dos")
    command_attack = generator.generate_attack_traffic("malicious_command")
    
    all_test_packets = normal_test_packets + recon_attack + dos_attack + command_attack
    
    # éªŒè¯æ‰€æœ‰æµ‹è¯•åŒ…
    valid_comm_count = 0
    invalid_comm_count = 0
    valid_value_count = 0
    invalid_value_count = 0
    
    for packet_meta, proto_data in all_test_packets:
        # æµ‹è¯•é€šä¿¡éªŒè¯
        comm_result = comm_learner.validate(packet_meta, proto_data)
        if comm_result['approved']:
            valid_comm_count += 1
        else:
            invalid_comm_count += 1
        
        # æµ‹è¯•å€¼åŸŸéªŒè¯
        value_result = value_learner.validate(packet_meta, proto_data)
        if value_result['approved']:
            valid_value_count += 1
        else:
            invalid_value_count += 1
    
    print(f"âœ… éªŒè¯ç»“æœ:")
    print(f"   é€šä¿¡éªŒè¯: {valid_comm_count} æœ‰æ•ˆ, {invalid_comm_count} æ— æ•ˆ")
    print(f"   å€¼åŸŸéªŒè¯: {valid_value_count} æœ‰æ•ˆ, {invalid_value_count} æ— æ•ˆ")
    
    # ç”Ÿæˆå®Œæ•´å­¦ä¹ æŠ¥å‘Š
    report = {
        'system_info': {
            'version': '1.0',
            'generated_at': datetime.now().isoformat(),
            'total_runtime_seconds': (datetime.now() - context.start_time).total_seconds()
        },
        'learning_context': {
            'mode': context.mode,
            'duration_days': context.duration_days,
            'start_time': context.start_time.isoformat(),
            'end_time': datetime.now().isoformat()
        },
        'statistics': {
            'total_packets_processed': context.total_packets_processed,
            'total_sessions_observed': context.total_sessions_observed,
            'total_connections_approved': context.total_connections_approved
        },
        'communication_learning': {
            'total_connections': len(comm_learner.connection_observations),
            'approved_connections': context.total_connections_approved,
            'rejected_connections': len(comm_learner.connection_observations) - context.total_connections_approved,
            'approval_rate': context.total_connections_approved / len(comm_learner.connection_observations) if len(comm_learner.connection_observations) > 0 else 0,
            'avg_confidence': sum(c.confidence for c in comm_learner.connection_observations.values()) / len(comm_learner.connection_observations) if comm_learner.connection_observations else 0
        },
        'value_learning': {
            'total_parameters': len(value_learner.value_observations),
            'valid_models': len([v for v in value_learner.value_observations.values() if v.observation_count >= value_learner.min_value_observations]),
            'invalid_models': len([v for v in value_learner.value_observations.values() if v.observation_count < value_learner.min_value_observations]),
            'avg_observations_per_param': sum(v.observation_count for v in value_learner.value_observations.values()) / len(value_learner.value_observations) if value_learner.value_observations else 0,
            'std_dev_multiplier': value_learner.std_dev_multiplier
        },
        'validation_results': {
            'total_test_packets': len(all_test_packets),
            'normal_packets': len(normal_test_packets),
            'attack_packets': len(recon_attack + dos_attack + command_attack),
            'communication_validation': {
                'valid_packets': valid_comm_count,
                'invalid_packets': invalid_comm_count,
                'detection_rate': invalid_comm_count / len(all_test_packets) * 100 if len(all_test_packets) > 0 else 0
            },
            'value_validation': {
                'valid_packets': valid_value_count,
                'invalid_packets': invalid_value_count,
                'detection_rate': invalid_value_count / len(all_test_packets) * 100 if len(all_test_packets) > 0 else 0
            }
        }
    }
    
    report_path = "outputs/learning_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š å­¦ä¹ æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_path}")
    
    # ç”Ÿæˆæ”»å‡»æ£€æµ‹æŠ¥å‘Š
    attack_report = {
        'test_time': datetime.now().isoformat(),
        'whitelist_size': {
            'communication_rules': len(whitelist['communication_rules']),
            'value_rules': len(whitelist['value_rules'])
        },
        'attack_detection': {
            'total_attacks': len(recon_attack + dos_attack + command_attack),
            'detected_attacks': invalid_comm_count + invalid_value_count,  # æ”»å‡»åº”è¯¥è¢«æ£€æµ‹ä¸ºæ— æ•ˆ
            'detection_rate': (invalid_comm_count + invalid_value_count) / len(recon_attack + dos_attack + command_attack) * 100 if len(recon_attack + dos_attack + command_attack) > 0 else 0
        },
        'test_summary': 'ç³»ç»ŸæˆåŠŸä½¿ç”¨å­¦ä¹ åˆ°çš„ç™½åå•æ£€æµ‹å¤šç§æ”»å‡»'
    }
    
    attack_report_path = "outputs/attack_test_report.json"
    with open(attack_report_path, 'w', encoding='utf-8') as f:
        json.dump(attack_report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ›¡ï¸  æ”»å‡»æ£€æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ° {attack_report_path}")
    
    print("\nğŸ‰ å®Œæ•´è¿è¡Œå®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - ç™½åå•: {whitelist_path}")
    print(f"   - å­¦ä¹ æŠ¥å‘Š: {report_path}")
    print(f"   - æ”»å‡»æ£€æµ‹æŠ¥å‘Š: {attack_report_path}")

if __name__ == "__main__":
    run_complete_pipeline()